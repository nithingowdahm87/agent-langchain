from src.llm_clients.gemini_client import GeminiClient
from src.llm_clients.groq_client import GroqClient
from src.llm_clients.nvidia_client import NvidiaClient
from src.tools.file_ops import read_file, scan_directory, write_file

class DockerWriterA:
    def __init__(self):
        self.llm = GeminiClient()
    
    def generate(self, app_path: str, context: str = "") -> str:
        tree = scan_directory(app_path)
        try:
            guidelines = read_file("configs/guidelines/docker-guidelines.md")
        except Exception:
            guidelines = "No specific guidelines found."
    def generate(self, context: str) -> str:
        try:
            system = read_file("configs/prompts/system_master.md")
            task = read_file("configs/prompts/docker/writer_a.md")
        except Exception:
            system = "You are a DevOps Engineer."
            task = "Generate a Dockerfile."
            
        prompt = f"{system}\n\n{task}\n\nAPPLICATION CONTEXT:\n{context}"
        return self.llm.call(prompt)

class DockerWriterB:
    def __init__(self):
        self.llm = GroqClient()
        
    def generate(self, context: str) -> str:
        try:
            system = read_file("configs/prompts/system_master.md")
            task = read_file("configs/prompts/docker/writer_b.md")
        except Exception:
            system = "You are a Security Engineer."
            task = "Generate a secure Dockerfile."
            
        prompt = f"{system}\n\n{task}\n\nAPPLICATION CONTEXT:\n{context}"
        return self.llm.call(prompt)

class DockerWriterC:
    def __init__(self):
        self.llm = NvidiaClient()
        
    def generate(self, context: str) -> str:
        try:
            system = read_file("configs/prompts/system_master.md")
            task = read_file("configs/prompts/docker/writer_c.md")
        except Exception:
            system = "You are a Performance Engineer."
            task = "Generate an optimized Dockerfile."
            
        prompt = f"{system}\n\n{task}\n\nAPPLICATION CONTEXT:\n{context}"
        return self.llm.call(prompt)

class DockerReviewer:
    def __init__(self):
        from src.llm_clients.perplexity_client import PerplexityClient
        self.llm = PerplexityClient()
    
    def review_and_merge(self, docker_a: str, docker_b: str, docker_c: str, validation_report: str = "") -> tuple[str, str]:
        """
        Uses Perplexity AI to intelligently review all 3 Dockerfiles and select/combine the best.
        Has access to guidelines and best practices.
        """
        # Load guidelines
        try:
            guidelines = read_file("configs/guidelines/docker-guidelines.md")
        except Exception:
            guidelines = "No specific guidelines available."
        
        # Build comprehensive review prompt
        review_prompt = f"""
        You are an expert DevOps engineer reviewing 3 different Dockerfiles generated by AI models.
        
        GUIDELINES TO FOLLOW:
        {guidelines}
        
        VALIDATION REPORT (Linter Errors to Fix):
        {validation_report}
        
        DOCKERFILE A (Production-focused):
        ```dockerfile
        {docker_a}
        ```
        
        DOCKERFILE B (Security-focused):
        ```dockerfile
        {docker_b}
        ```
        
        DOCKERFILE C (Performance-focused):
        ```dockerfile
        {docker_c}
        ```
        
        YOUR TASK:
        1. Analyze all 3 Dockerfiles against the guidelines
        2. CRITICAL: Fix any errors mentioned in the VALIDATION REPORT.
        3. Identify strengths and weaknesses of each
        4. Select the best one OR combine the best elements from all three
        5. Explain WHY your choice is best with specific points
        
        OUTPUT FORMAT:
        First, provide 3-5 bullet points explaining why your selection is best.
        Then provide the final Dockerfile.
        
        Example:
        REASONING:
        - Multi-stage build reduces final image size by 60%
        - Non-root user improves security posture
        - Health checks enable auto-healing in production
        - Specific version pinning ensures reproducibility
        - Fixed hadolint error DL3008 by pinning apt-get install version
        
        DOCKERFILE:
        FROM node:18-alpine AS builder
        ...
        
        Provide your response in this exact format.
        """.strip()
        
        # Get AI review
        try:
            response = self.llm.call(review_prompt)
            
            # Parse response
            if "DOCKERFILE:" in response:
                parts = response.split("DOCKERFILE:", 1)
                reasoning = parts[0].replace("REASONING:", "").strip()
                dockerfile = parts[1].strip()
            else:
                # Fallback if format not followed
                reasoning = "AI review completed"
                dockerfile = response
            
            # Clean up any markdown artifacts
            dockerfile = dockerfile.replace("```dockerfile", "").replace("```", "").strip()
            
            return (dockerfile + "\n", reasoning)
            
        except Exception as e:
            print(f"Warning: AI review failed ({e}), falling back to longest valid output")
            # Fallback to deterministic logic if AI fails
            candidates = []
            if "FROM" in docker_a: candidates.append(docker_a)
            if "FROM" in docker_b: candidates.append(docker_b)
            if "FROM" in docker_c: candidates.append(docker_c)
            if not candidates:
                return ("# No valid Dockerfile generated.\n", "No valid outputs to review")
            best = max(candidates, key=len).strip() + "\n"
            return (best, "Fallback: Selected longest valid Dockerfile")

class DockerExecutor:
    def run(self, final_dockerfile: str, project_path: str, output_path: str = "Dockerfile") -> None:
        write_file(f"{project_path}/{output_path}", final_dockerfile)
        print(f"Wrote {output_path}. Next manual step:")
        print(f"  docker build -t myapp:local -f {project_path}/{output_path} {project_path}")
